{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "name": "RMSprop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWDDerUiebHP"
      },
      "source": [
        "# Drive Mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5XJ2Xb0YKf"
      },
      "source": [
        "# Mounting Google drive to handle data read and write operations\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "% cd 'drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQaojgtufU7k"
      },
      "source": [
        "# Import Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZQuG9i74Sa"
      },
      "source": [
        "# Import all the libraries required later  \n",
        "\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import *\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import pickle \n",
        "from tqdm.notebook import tqdm\n",
        "from zipfile import ZipFile\n",
        "from glob import glob\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8822L5-uUMF"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixTicekVgXqg"
      },
      "source": [
        "\"\"\"\n",
        "  This part create form for all the configurations required which are:\n",
        "    1- Optimizer Selection: RMSProp, Adam, AdaGrad --> (test_name)\n",
        "    2- Batch Size --> (batch_size)\n",
        "    3- High Resolution Image Height --> (hr_height)\n",
        "    4- High Resolution Image Width --> (hr_width)\n",
        "    5- Scaling Factor --> (scale)\n",
        "    6- Reset Checkpoints: to start training from begining if set True --> (RESET_CHENCKPOINTS)\n",
        "    7- Training Dataset Path --> (training_path)\n",
        "    8- Number of Epochs --> (iterations)\n",
        "    9- Evaluation or Validation interval --> (evaluation_interval)\n",
        "    10- Models Saving Interval --> (interval)\n",
        "    11- Generator Learning Rate --> (gen_lr)\n",
        "    12- Discriminator Learning Rate --> (disc_lr)\n",
        "    13- Generator Number of Training Times --> (K)\n",
        "    14- Adversarail Loss Weight __ alpha --> (ad_loss_weight)\n",
        "    15- Generator Number of Filter --> (gf)\n",
        "    16- Generator Kernel Size --> (gk)\n",
        "    17- Number of Channles __ grayscale --> (channels)\n",
        "    18- Total Number of Image --> total_images\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRoVbScMt6Qg",
        "cellView": "form"
      },
      "source": [
        "#@title Config\n",
        "test_name = \"RMSprop\" #@param [\"Adam\", \"RMSprop\", \"Adagrad\"]\n",
        "batch_size =   32#@param {type:\"number\"}\n",
        "hr_height =  256 #@param {type:\"number\"}\n",
        "hr_width =  256 #@param {type:\"number\"}\n",
        "scale = 4 #@param {type:\"number\"}\n",
        "RESET_CHECKPOINTS = False #@param {type:\"boolean\"}\n",
        "training_path= '/content/Data/' #@param {type:\"string\"}\n",
        "\n",
        "#@title training parameters\n",
        "iterations = 100000 #@param {type:\"number\"}\n",
        "evaluation_interval =  5 #@param {type:\"number\"}\n",
        "interval= 10       #@param {type:\"number\"}\n",
        "gen_lr = 1e-4 #@param {type:\"number\"}\n",
        "disc_lr = 1e-4 #@param {type:\"number\"}\n",
        "K =  1#@param {type:\"number\"}\n",
        "\n",
        "ad_loss_weight = 1e-3 #@param {type:\"number\"}\n",
        "gf = 32 #@param {type:\"number\"}\n",
        "gk =  4#@param {type:\"number\"}\n",
        "\n",
        "#@title image parameters\n",
        "channels  = 1 #@param {type:\"number\"}\n",
        "scale = 4 #@param {type:\"number\"}\n",
        "total_images = 100000 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "train_len = total_images*3//4 \n",
        "val_len = total_images//4\n",
        "\n",
        "hr_shape = (hr_height , hr_width , channels)\n",
        "lr_height = hr_height // scale\n",
        "lr_width  = hr_width // scale\n",
        "lr_shape  = (lr_height , lr_width , channels )\n",
        "total = train_len//batch_size +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-22-3x0jf-a"
      },
      "source": [
        "# Prerequisite Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuZK7YwV2Lh9"
      },
      "source": [
        "\"\"\" \n",
        "  Create directories for:\n",
        "    - Sample images\n",
        "    - Checkpoints\n",
        "    - Models\n",
        "\"\"\"\n",
        "\n",
        "dir_path= './samples/%s'% test_name\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "dir_path= './checkpoints/%s'% test_name\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "dir_path= './models/%s'% test_name\n",
        "os.makedirs(dir_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqEtP89aE1uH"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nvLqtb7WBPm"
      },
      "source": [
        "ex_discriptoin = { \n",
        "    'hr': tf.io.FixedLenFeature((), tf.string),\n",
        "    'lr': tf.io.FixedLenFeature((), tf.string)\n",
        "}\n",
        "\n",
        "def parse_example(ex): \n",
        "  example = tf.io.parse_single_example(ex, ex_discriptoin)\n",
        "  hr = example['hr']\n",
        "  lr = example['lr']\n",
        "  hr = tf.io.decode_jpeg(hr, channels=1)\n",
        "  lr = tf.io.decode_jpeg(lr, channels=1)\n",
        "  return hr, lr\n",
        "\n",
        "def normalize(hr, lr): \n",
        "  lr = lr/255\n",
        "  hr = hr/255 \n",
        "  hr = 2*hr-1\n",
        "  return hr, lr\n",
        "\n",
        "record_files = glob('/content/drive/My Drive/tfrecords_data/*')\n",
        "record_ds = tf.data.TFRecordDataset(record_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "val_ds = record_ds.take(val_len)\n",
        "val_ds = val_ds.map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_ds = record_ds.skip(val_len).take(train_len)\n",
        "train_ds = train_ds.map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbS3GHFJ2RfB"
      },
      "source": [
        "def take_elements(n): \n",
        "    hr= []\n",
        "    lr = []\n",
        "    for i, j in train_ds.shuffle(1000).take(n):\n",
        "        hr.append(i)\n",
        "        lr.append(j)\n",
        "    return tf.convert_to_tensor(hr), tf.convert_to_tensor(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj6-y84jKtc-"
      },
      "source": [
        "# Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kplRL-Lt2o72"
      },
      "source": [
        "def compute_metrics(hr, fake):\n",
        "   \n",
        "    # hr and fake must be in range [0,1]\n",
        "    \n",
        "    psnr = tf.image.psnr(hr, fake, 1)\n",
        "    ssim = tf.image.ssim(hr, fake, 1) \n",
        "    \n",
        "    return psnr, ssim\n",
        "\n",
        "def sample_image(epoch, test_name=test_name, save_sample=True): \n",
        "        r, c = 2, 2\n",
        "        imgs_hr, imgs_lr = take_elements(2)\n",
        "\n",
        "        fake_hr = G(imgs_lr)  # output in range [-1, 1]\n",
        "        \n",
        "        ######################calculate metrics###########\n",
        "        fake_hr = 0.5*fake_hr +0.5  # range 0, 1\n",
        "        imgs_hr = 0.5*imgs_hr +0.5  # range 0, 1\n",
        "        \n",
        "        psnr, ssim =compute_metrics(fake_hr, imgs_hr)\n",
        "        \n",
        "        print('PSNR= ', np.mean(psnr))\n",
        "        print('SSIM= ', np.mean(ssim))\n",
        "        \n",
        "        # Save generated images and the high resolution originals\n",
        "        titles = ['Generated', 'Original']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        figSize = fig.get_size_inches()*4\n",
        "        fig.set_size_inches(figSize)\n",
        "        cnt = 0\n",
        "        for row in range(r):\n",
        "            for col, image in enumerate([fake_hr, imgs_hr]):\n",
        "                axs[row, col].imshow(image[row,:,:,0] , cmap='gray')\n",
        "                axs[row, col].set_title(titles[col])\n",
        "                axs[row, col].axis('off')\n",
        "            cnt += 1\n",
        "\n",
        "        if save_sample:     \n",
        "          fig.savefig(\"samples/%s/%d.png\" % (test_name, epoch))\n",
        "          # Save low resolution images for comparison\n",
        "          plt.imsave(arr= fake_hr[0,:,:,0],fname=('samples/%s/%d_generated.jpg' % (test_name, epoch)), cmap='gray' )\n",
        "          plt.imsave(arr= imgs_hr[0,:,:,0],fname=('samples/%s/%d_real.jpg' % (test_name, epoch) ), cmap='gray' )\n",
        "        else: \n",
        "          plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX3FAvgNcQtz"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP-3pF6M8fXg"
      },
      "source": [
        "## Build Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOfNEc_E8aSK"
      },
      "source": [
        "def pixel_shuffle(scale): \n",
        "  return lambda x: tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "\n",
        "def upsample(x_in, num_filters): \n",
        "  lyr = Conv2D(num_filters, 4, padding='same')(x_in)\n",
        "  lyr = Lambda(pixel_shuffle(scale=2))(lyr)\n",
        "  return LeakyReLU()(lyr)\n",
        "\n",
        "def RRDB(lyr, name): \n",
        "  layer1 = Conv2D(gf*2, 3, strides=1, padding='same', name=f'a_{name}')(lyr)\n",
        "  layer1 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer1)\n",
        "  layer1 = LeakyReLU()(layer1)\n",
        "  tmp = Add()([lyr, layer1])\n",
        "  layer2 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'b_{name}')(tmp)\n",
        "  layer2 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer2)\n",
        "  layer2 = LeakyReLU()(layer2)\n",
        "  tmp = Add()([lyr, layer1, layer2])\n",
        "  layer3 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'c_{name}')(tmp)\n",
        "  layer3 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer3)\n",
        "  layer3 = LeakyReLU()(layer3)\n",
        "  tmp = Add()([lyr, layer1, layer2, layer3])\n",
        "  layer4 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'd_{name}')(tmp)\n",
        "  layer4 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer4)\n",
        "  layer4 = LeakyReLU()(layer4)\n",
        "  return Add()([lyr, layer1, layer2, layer3, layer4])\n",
        "def res_block(pre_layer):\n",
        "    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(pre_layer)\n",
        "    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n",
        "    lyr = LeakyReLU()(lyr)\n",
        "\n",
        "    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(lyr)\n",
        "    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n",
        "    lyr = LeakyReLU()(lyr)\n",
        "\n",
        "\n",
        "    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(lyr)\n",
        "    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n",
        "    lyr = LeakyReLU()(lyr)\n",
        "    return Add()([lyr, pre_layer])\n",
        "\n",
        "  \n",
        "\n",
        "def build_G():  \n",
        "\n",
        "    input_layer = Input(shape= (None, None, channels)) \n",
        "    \n",
        "    tmp = Conv2D(gf*2, (3,3) , padding='same', strides=1)(input_layer)\n",
        "    # extracting basic details:\n",
        "      ####  Branch A \n",
        "    branch_A =  Conv2D(gf, (3,3) , padding='same', strides=1)(input_layer)\n",
        "    branch_A = res_block(branch_A) \n",
        "    branch_A = res_block(branch_A)\n",
        "    branch_A = res_block(branch_A)\n",
        "      #### Branch B\n",
        "    branch_B =  Conv2D(gf, (3,3) , padding='same', strides=1)(input_layer)\n",
        "    branch_B = res_block(branch_B) \n",
        "    branch_B = res_block(branch_B)\n",
        "    branch_B = res_block(branch_B)\n",
        "\n",
        "    layer = Concatenate()([branch_A, branch_B])\n",
        "    layer = RRDB(layer, '1')\n",
        "    layer = RRDB(layer, '2')\n",
        "\n",
        "    layer = Add()([tmp, layer])\n",
        "    layer = upsample(layer, gf*4)\n",
        "    layer = upsample(layer, gf*4)\n",
        "    layer = Conv2D(1, 9, padding='same', activation='tanh')(layer)\n",
        "    return tf.keras.models.Model(input_layer, layer)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfyyL2sM8i7H"
      },
      "source": [
        "## Build Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAiHfJpi_MS-"
      },
      "source": [
        "def build_D():\n",
        "    # seems to have vanishing gradients , try to replace relu with LeakyRelu\n",
        "    input_layer= Input(shape=hr_shape)\n",
        "    \n",
        "    layer = Conv2D(64, 3, strides=1)(input_layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64, 3, strides=2)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*2, 3, strides=1)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*2, 3, strides=2)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*4, 3, strides=1)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*4, 3, strides=2)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*8, 3, strides=1)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer = Conv2D(64*8, 3, strides=2)(layer)\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    \n",
        "    layer= Flatten()(layer)\n",
        "    layer = Dense(1024)(layer)\n",
        "    layer = LeakyReLU(0.2)(layer)\n",
        "    output_layer = Dense(1)(layer)\n",
        "    return tf.keras.models.Model(input_layer, output_layer) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuvfjRQC2yBq"
      },
      "source": [
        "# Checkpoints config\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XCdxRaP239X"
      },
      "source": [
        "def save_checkpoint(step_num, psnr_list, ssim_list, test_name=test_name):\n",
        "    G.save('./checkpoints/{}/generator.h5'.format(test_name))\n",
        "    D.save('./checkpoints/{}/discriminator.h5'.format(test_name))\n",
        "    saving_file= open('./checkpoints/{}/steps_psnr_ssim.pickle'.format(test_name) , 'wb')\n",
        "    pickle.dump((step_num, psnr_list, ssim_list) ,saving_file)\n",
        "    saving_file.close()\n",
        "\n",
        "    G.save(f'./checkpoints/{test_name}/generator_old.h5')\n",
        "    D.save(f'./checkpoints/{test_name}/discriminator_old.h5')\n",
        "    print('#####################checkpoint saved#########################')\n",
        "\n",
        "def load_checkpoint(test_name=test_name):\n",
        "    try:\n",
        "      g=tf.keras.models.load_model(f'./checkpoints/{test_name}/generator.h5')\n",
        "    except OSError:\n",
        "      g=tf.keras.models.load_model(f'./checkpoints/{test_name}/generator_old.h5')\n",
        "    try:\n",
        "      d=tf.keras.models.load_model('./checkpoints/{}/discriminator.h5'.format(test_name))\n",
        "    except OSError:\n",
        "      d=tf.keras.models.load_model('./checkpoints/{}/discriminator_old.h5'.format(test_name))\n",
        "      \n",
        "    saving_file= open('./checkpoints/{}/steps_psnr_ssim.pickle'.format(test_name) , 'rb')\n",
        "    step, psnr_list, ssim_list=pickle.load(saving_file)\n",
        "    saving_file.close()\n",
        "    return  step, psnr_list, ssim_list , g, d\n",
        "   \n",
        "\n",
        "def reset_checkpoint(test_name=test_name): \n",
        "  \n",
        "  dir_path= './samples/%s'% test_name\n",
        "  if os.path.exists(dir_path): \n",
        "    shutil.rmtree(dir_path)\n",
        "    os.mkdir(dir_path)\n",
        "  dir_path= './checkpoints/%s'% test_name\n",
        "  if os.path.exists(dir_path): \n",
        "    shutil.rmtree(dir_path)\n",
        "    os.mkdir(dir_path)\n",
        "  dir_path= './models/%s'% test_name\n",
        "  if os.path.exists(dir_path): \n",
        "    shutil.rmtree(dir_path)\n",
        "    os.mkdir(dir_path)\n",
        "\n",
        "  \n",
        "  save_checkpoint(0, [], [], test_name)\n",
        "  print('reset checkpoint')\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LE4fSvX3DI4"
      },
      "source": [
        "### Load checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM4aSvH-3CQT"
      },
      "source": [
        "if RESET_CHECKPOINTS: \n",
        "  G = build_G()\n",
        "  D = build_D()\n",
        "  reset_checkpoint()\n",
        "\n",
        "last_epoch, psnr_list, ssim_list, G, D= load_checkpoint(test_name)\n",
        "print(last_epoch, psnr_list, ssim_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZARNpWJ-E7aG"
      },
      "source": [
        "# Losses and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENS4WqWAPSN"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "def disc_loss(real_logits, fake_logits): \n",
        "    real_labels = tf.zeros_like(real_logits) + 0.05*tf.random.normal(real_logits.shape,mean=0)\n",
        "    fake_labels = tf.ones_like(fake_logits) + 0.05*tf.random.normal(fake_logits.shape,mean=0)\n",
        "    real_loss = cross_entropy(real_labels, real_logits)\n",
        "    fake_loss = cross_entropy(fake_labels, fake_logits)\n",
        "    return 0.5*(real_loss+fake_loss)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERc81GeoAhag"
      },
      "source": [
        "def gen_loss(fake_logits): \n",
        "    # fake logits is the discriminator's decision  about the images came from the generator\n",
        "    return cross_entropy(tf.zeros_like(fake_logits), fake_logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmbdcKIu51pN"
      },
      "source": [
        "## Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI9WJDz_FCYZ"
      },
      "source": [
        "if test_name == 'Adam': \n",
        "  disc_opt = tf.keras.optimizers.Adam(disc_lr)\n",
        "  gen_opt = tf.keras.optimizers.Adam(gen_lr)\n",
        "\n",
        "elif test_name == 'Adagrad': \n",
        "  disc_opt = tf.keras.optimizers.Adagrad(disc_lr)\n",
        "  gen_opt = tf.keras.optimizers.Adagrad(gen_lr)\n",
        "\n",
        "elif test_name=='RMSprop':\n",
        "  disc_opt = tf.keras.optimizers.RMSprop(disc_lr)\n",
        "  gen_opt = tf.keras.optimizers.RMSprop(gen_lr)\n",
        "\n",
        "else: \n",
        "  print(\"Error optimizer is not set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMqD3eDQFDVA"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIp9_3ccAfGf"
      },
      "source": [
        "def train_gen(hr, lr):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        fake= G(lr, training=True)\n",
        "        fake_logits = D(fake, training=False)\n",
        "        content_loss = mse_loss_fn(hr, fake)\n",
        "        adv_loss = gen_loss(fake_logits)\n",
        "        loss = content_loss + ad_loss_weight*adv_loss\n",
        "    \n",
        "    gen_grads= tape.gradient(loss, G.trainable_variables)\n",
        "    gen_opt.apply_gradients(zip(gen_grads, G.trainable_variables))\n",
        "    adv_grads = tape.gradient(adv_loss, G.trainable_variables)\n",
        "\n",
        "    return {\n",
        "        'content_loss': tf.math.reduce_mean(content_loss), \n",
        "        'adv_loss': tf.math.reduce_mean(adv_loss), \n",
        "        'deep_grads': np.mean(gen_grads[-5]),\n",
        "        'shallow_grads': np.mean(gen_grads[4]),\n",
        "        'adv_deep_grads': np.mean(adv_grads[-3]),\n",
        "        'adv_shallow_grads': np.mean(adv_grads[0])\n",
        "    }\n",
        "        \n",
        "    \n",
        "    \n",
        "def train_disc(hr, lr):\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake= G(lr, training=False)\n",
        "        real_logits = D(hr, training=True)\n",
        "        fake_logits = D(fake, training=True)\n",
        "        loss = disc_loss(real_logits, fake_logits)\n",
        "    \n",
        "    disc_grads = tape.gradient(loss, D.trainable_variables)\n",
        "    disc_opt.apply_gradients(zip(disc_grads, D.trainable_variables))\n",
        "    \n",
        "    return {\n",
        "        'loss': tf.math.reduce_mean(loss),\n",
        "        'deep_grads': np.mean(disc_grads[-1]), \n",
        "        'shallow_grads': np.mean(disc_grads[0])\n",
        "        \n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXJJxkTds_LE"
      },
      "source": [
        "# Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHSxwhYBs-hj"
      },
      "source": [
        "def evaluate():\n",
        "    adv_loss= 0\n",
        "    d_loss= 0\n",
        "    content_loss = 0\n",
        "    psnr = []\n",
        "    ssim =[] \n",
        "    step = 0\n",
        "    for batch_hr, batch_lr in tqdm(val_ds.batch(batch_size), total= val_len//batch_size, unit=' batch',desc=f'Evaluating: '):\n",
        "        fake= G(batch_lr, training=False)\n",
        "        fake_logits = D(fake, training=False)\n",
        "   \n",
        "        c_loss = mse_loss_fn(batch_hr, fake)\n",
        "        c_loss = tf.math.reduce_mean(c_loss)\n",
        "        \n",
        "        a_loss = gen_loss(fake_logits)\n",
        "        t_loss = c_loss + ad_loss_weight*a_loss\n",
        "\n",
        "        real_logits = D(batch_hr, training=False)\n",
        "        d_loss = disc_loss(real_logits, fake_logits)\n",
        "        \n",
        "        adv_loss += tf.math.reduce_mean(a_loss)\n",
        "        d_loss += tf.math.reduce_mean(d_loss)\n",
        "        content_loss += tf.math.reduce_mean(c_loss)\n",
        "        \n",
        "        step +=1\n",
        "      \n",
        "           \n",
        "        fake= fake*0.5+0.5\n",
        "        batch_hr= batch_hr*0.5+0.5\n",
        "        p, s= compute_metrics(batch_hr, fake)\n",
        "        p = tf.math.reduce_mean(p)\n",
        "        s = tf.math.reduce_mean(s)\n",
        "\n",
        "        psnr.append(p)\n",
        "        ssim.append(s)\n",
        "          \n",
        "        \n",
        "    print(\"*-\"*25)\n",
        "    mean_psnr = np.mean(psnr)\n",
        "    mean_ssim = np.mean(ssim)\n",
        "    adv_loss_mean = adv_loss/total\n",
        "    disc_loss_mean = d_loss/total\n",
        "    content_loss_mean = content_loss/total\n",
        "    print(\"*-\"*25)\n",
        "    print(\"disc loss = {:.3f} adv loss={:.3f}  content_loss = {:.3f}\\nOverall PSNR: {:.3f}, SSIM: {:.3f}\".format(disc_loss_mean,  adv_loss_mean,content_loss_mean, mean_psnr, mean_ssim))    \n",
        "    print(\"*-\"*25)\n",
        "\n",
        "\n",
        "    #############saving checkpoint#################\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhoAGnxFJjR"
      },
      "source": [
        "# **Training Loops**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1VmTOIBw_8"
      },
      "source": [
        "for epoch in range(last_epoch, iterations):\n",
        "    adv_loss= 0\n",
        "    d_loss= 0\n",
        "    content_loss = 0\n",
        "    psnr = []\n",
        "    ssim =[] \n",
        "    adv_sh_grads = 0 \n",
        "    adv_deep_grads = 0 \n",
        "    gen_sh_grads = 0 \n",
        "    gen_deep_grads = 0 \n",
        "    disc_sh_grads = 0 \n",
        "    disc_deep_grads = 0 \n",
        "    step = 0\n",
        "    k = K\n",
        "    checkout_interval = total//10\n",
        "    for batch_hr, batch_lr in tqdm(train_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), total= total, unit=' batch',desc=f'Epoch {epoch}'):\n",
        "        \n",
        "        d = train_disc(batch_hr, batch_lr)\n",
        "        if k == K:\n",
        "          g = train_gen(batch_hr, batch_lr)\n",
        "          k = 0\n",
        "        adv_loss += g['adv_loss']\n",
        "        d_loss += d['loss']\n",
        "        content_loss += (g['content_loss'])\n",
        "        adv_sh_grads += g['adv_shallow_grads']\n",
        "        adv_deep_grads += g['adv_deep_grads']\n",
        "        gen_sh_grads += g['shallow_grads']\n",
        "        gen_deep_grads += g['deep_grads']\n",
        " \n",
        "        disc_sh_grads += d['shallow_grads']\n",
        "        disc_deep_grads += d['deep_grads']\n",
        "        k+=1\n",
        "        step +=1\n",
        "        if step%(checkout_interval)== 0 : \n",
        "            fake= G(batch_lr, training=False)\n",
        "            fake= fake*0.5+0.5\n",
        "            batch_hr= batch_hr*0.5+0.5\n",
        "            p, s= compute_metrics(batch_hr, fake)\n",
        "            psnr.append(p)\n",
        "            ssim.append(s)\n",
        "            print(f'\\n adv_loss: {adv_loss/step:.4f} content_loss: {content_loss/step:.4f}  disc_loss: {d_loss/step:.4f} \\n psnr: {np.mean(p):.4f}, ssim: {np.mean(s):.4f}')\n",
        "            # sample_image(epoch, test_name, save_sample=False)\n",
        "            # save_checkpoint(epoch, psnr_list, ssim_list, test_name)\n",
        "        \n",
        "    print(\"*-\"*25)\n",
        "    print('GRADIENTS:')\n",
        "    \n",
        "    print('Gen gradients: shallow: {}   deep:{}'.format(gen_sh_grads/step, gen_sh_grads/step))\n",
        "    print('disc gradients: shallow: {}   deep:{}'.format(disc_sh_grads/step, disc_sh_grads/step))\n",
        "    print('adv gradients: shallow: {}   deep:{}'.format(adv_sh_grads/step, adv_sh_grads/step))\n",
        "    mean_psnr = np.mean(psnr)\n",
        "    mean_ssim = np.mean(ssim)\n",
        "    adv_loss_mean = adv_loss/total\n",
        "    disc_loss_mean = d_loss/total\n",
        "    content_loss_mean = content_loss/total\n",
        "    print(\"*-\"*25)\n",
        "    print(\"disc loss = {:.3f} adv loss={:.3f}  content_loss = {:.3f}\\nOverall PSNR: {:.3f}, SSIM: {:.3f}\".format(disc_loss_mean,  adv_loss_mean,content_loss_mean, mean_psnr, mean_ssim))    \n",
        "    print(\"*-\"*25)\n",
        "\n",
        "    #############saving checkpoint#################\n",
        "    \n",
        "    psnr_list.append(mean_psnr)\n",
        "    ssim_list.append(mean_ssim) \n",
        "    save_checkpoint(epoch+1, psnr_list, ssim_list, test_name)\n",
        "    if epoch%interval == 0 :\n",
        "        G.save('./models/{}/G_{}.h5'.format(test_name, epoch))\n",
        "        D.save('./models/{}/D_{}.h5'.format(test_name, epoch))\n",
        "\n",
        "    if epoch%evaluation_interval==0:\n",
        "      evaluate()\n",
        "    sample_image(epoch, test_name, save_sample=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}